<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <script>let thunk=()=>{let t=t=>t.trim(),e=t=>t.innerText,n=t=>{let e=t.split(" "),n=e.slice(0,-1).join(" ");return[e.at(-1),n]},a=Array.from(document.getElementsByClassName("author")).map(e).map(t).map(n),o=a[0][0],r=(Array.from(document.getElementsByClassName("affiliation")).filter(t=>"P"===t.nodeName).map(e).map(t),"April 28, 2025"),i="An Illustrated Guide to Automatic Sparse Differentiation",l="Your blog post's abstract. Please add your abstract or summary here and not in the main body of your text. Do not include math/latex or hyperlinks.";{let t=a.map(t=>`${t[0]}, ${t[1]}`).join(" and "),e=`\n@inproceedings{${(o+"2025"+i.split(" ").slice(0,3).join("")).replace(" ","").replace(/[\p{P}$+<=>^`|~]/gu,"").toLowerCase().trim()},\n  author = {${t}},\n  title = {${i}},\n  abstract = {${l}},\n  booktitle = {ICLR Blogposts 2025},\n  year = {2025},\n  date = {${r}},\n  note = {${window.location.href}},\n  url  = {${window.location.href}}\n}\n  `.trim();document.getElementById("bibtex-box").innerText=e}{let t=a.map(t=>t[0]),e=`\n${t=t.length>2?t[0]+", et al.":2==t.length?t[0]+" & "+t[1]:t[0]}, "${i}", ICLR Blogposts, 2025.\n`.trim();document.getElementById("bibtex-academic-attribution").innerText=e}};document.addEventListener("readystatechange",function(){"complete"===document.readyState&&thunk()});</script> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>An Illustrated Guide to Automatic Sparse Differentiation | ICLR Blogposts 2025</title> <meta name="author" content="ICLR Blog"> <meta name="description" content="Your blog post's abstract. Please add your abstract or summary here and not in the main body of your text. Do not include math/latex or hyperlinks."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, iclr"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/iclr2025-sparse-autodiff/assets/img/iclr_favicon.ico"> <link rel="stylesheet" href="/iclr2025-sparse-autodiff/assets/css/main.css"> <link rel="canonical" href="https://adrhill.github.io/iclr2025-sparse-autodiff/blog/sparse-autodiff/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/iclr2025-sparse-autodiff/assets/js/theme.js"></script> <script src="/iclr2025-sparse-autodiff/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/iclr2025-sparse-autodiff/assets/js/distillpub/template.v2.js"></script> <script src="/iclr2025-sparse-autodiff/assets/js/distillpub/transforms.v2.js"></script> <script src="/iclr2025-sparse-autodiff/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:##bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> <d-front-matter> <script async type="text/json">{
      "title": "An Illustrated Guide to Automatic Sparse Differentiation",
      "description": "Your blog post's abstract. Please add your abstract or summary here and not in the main body of your text. Do not include math/latex or hyperlinks.",
      "published": "April 28, 2025",
      "authors": [
        {
          "author": "Anonymous",
          "authorURL": "",
          "affiliations": [
            {
              "name": "",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> </head> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/iclr2025-sparse-autodiff/">ICLR Blogposts 2025</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/iclr2025-sparse-autodiff/about/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/iclr2025-sparse-autodiff/call/">call for blogposts</a> </li> <li class="nav-item "> <a class="nav-link" href="/iclr2025-sparse-autodiff/submitting/">submitting</a> </li> <li class="nav-item "> <a class="nav-link" href="/iclr2025-sparse-autodiff/reviewing/">reviewing</a> </li> <li class="nav-item "> <a class="nav-link" href="/iclr2025-sparse-autodiff/blog/index.html">blog</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2025/" rel="external nofollow noopener noopener noreferrer" target="_blank"><strong>2025</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2024/" rel="external nofollow noopener noopener noreferrer" target="_blank">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2023/" rel="external nofollow noopener noopener noreferrer" target="_blank">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener noopener noreferrer" target="_blank">2022</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>An Illustrated Guide to Automatic Sparse Differentiation</h1> <p>Your blog post's abstract. Please add your abstract or summary here and not in the main body of your text. Do not include math/latex or hyperlinks.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#automatic-differentiation">Automatic differentiation</a></div> <ul> <li><a href="#the-chain-rule">The chain rule</a></li> <li><a href="#ad-is-matrix-free">AD is matrix-free</a></li> <li><a href="#forward-mode-ad">Forward-mode AD</a></li> <li><a href="#reverse-mode-ad">Reverse-mode AD</a></li> </ul> <div><a href="#automatic-sparse-differentiation">Automatic sparse differentiation</a></div> <ul> <li><a href="#sparse-matrices">Sparse matrices</a></li> <li><a href="#leveraging-structure">Leveraging structure</a></li> <li><a href="#sparsity-pattern-detection-and-coloring">Sparsity pattern detection and coloring</a></li> </ul> <div><a href="#pattern-detection">Pattern detection</a></div> <ul> <li><a href="#compressing-jacobians">Compressing Jacobians</a></li> <li><a href="#propagating-index-sets">Propagating index sets</a></li> <li><a href="#alternative-evaluation">Alternative evaluation</a></li> </ul> <div><a href="#matrix-coloring">Matrix coloring</a></div> <div><a href="#second-order-sparse-differentiation">Second-order sparse differentiation</a></div> <div><a href="#demonstration">Demonstration</a></div> </nav> </d-contents> <div style="display: none"> $$ \newcommand{\colorf}[1]{\textcolor{RoyalBlue}{#1}} \newcommand{\colorh}[1]{\textcolor{RedOrange}{#1}} \newcommand{\colorg}[1]{\textcolor{PineGreen}{#1}} \newcommand{\colorv}[1]{\textcolor{VioletRed}{#1}} \def\sR{\mathbb{R}} \def\vx{\mathbf{x}} \def\vv{\mathbf{v}} \def\vb{\mathbf{e}} \newcommand{\vvc}[1]{\colorv{\vv_{#1}}} \newcommand{\vbc}[1]{\colorv{\vb_{#1}}} \newcommand{\dfdx}[2]{\frac{\partial f_{#1}}{\partial x_{#2}}(\vx)} \newcommand{\J}[2]{J_{#1}(#2)} \def\Jf{\J{f}{\vx}} \def\Jg{\J{g}{\vx}} \def\Jh{\J{h}{g(\vx)}} \def\Jfc{\colorf{\Jf}} \def\Jgc{\colorg{\Jg}} \def\Jhc{\colorh{\Jh}} \newcommand{\D}[2]{D{#1}(#2)} \def\Df{\D{f}{\vx}} \def\Dg{\D{g}{\vx}} \def\Dh{\D{h}{g(\vx)}} \def\Dfc{\colorf{\Df}} \def\Dgc{\colorg{\Dg}} \def\Dhc{\colorh{\Dh}} $$ </div> <p>First-order optimization is ubiquitous in Machine Learning (ML) but second-order optimization is much less common. The intuitive reason is that large gradients are cheap, whereas large Hessian matrices are expensive. Luckily, in numerous applications of ML to science or engineering, <strong>Hessians (and Jacobians) exhibit sparsity</strong>: most of their coefficients are known to be zero. Leveraging this sparsity can vastly <strong>accelerate Automatic Differentiation</strong> (AD) for Hessians and Jacobians, while decreasing its memory requirements. Yet, while traditional AD is available in many high-level programming languages, <strong>automatic sparse differentiation (ASD) is not as widely used</strong>. One reason is that the underlying theory was developed outside of the ML research ecosystem, by people more familiar with low-level programming languages.</p> <p>With this blog post, we aim to shed light on the inner workings of ASD, thus bridging the gap between the ML and AD communities. We start out with a short introduction to traditional AD, covering the computation of Jacobians in both forward and reverse mode. We then dive into the two primary components of ASD: <strong>sparsity pattern detection</strong> and <strong>matrix coloring</strong>. Having described the computation of sparse Jacobians, we move on to sparse Hessians.<br> We conclude with a practical demonstration of ASD, providing performance benchmarks and guidance on when to use ASD over AD.</p> <h2 id="automatic-differentiation">Automatic Differentiation</h2> <p>Let us start by covering the fundamentals of traditional AD.</p> <p>AD makes use of the <strong>compositional structure</strong> of mathematical functions like deep neural networks. To make things simple, we will mainly look at a differentiable function $f$ composed of two differentiable functions $g: \sR^{n} \rightarrow \sR^{p}$ and $h: \sR^{p} \rightarrow \sR^{m}$, such that $f = h \circ g: \sR^{n} \rightarrow \sR^{m}$. The insights gained from this toy example should translate directly to more deeply composed functions $f = g^{(L)} \circ g^{(L-1)} \circ \cdots \circ g^{(1)}$. For ease of visualization, we work in small dimension, but the real benefits of ASD only appear as the dimension grows.</p> <h3 id="the-chain-rule">The chain rule</h3> <p>For a function $f: \sR^{n} \rightarrow \sR^{m}$ and a point of linearization $\vx \in \sR^{n}$, the Jacobian $J_f(\vx)$ is the $m \times n$ matrix of first-order partial derivatives, such that the $(i,j)$-th entry is</p> \[\big( \Jf \big)_{i,j} = \dfdx{i}{j} \in \sR \quad .\] <p>For a composed function</p> \[\colorf{f} = \colorh{h} \circ \colorg{g},\] <p>the <strong>multivariate chain rule</strong> tells us that we obtain the Jacobian of $f$ by <strong>multiplying</strong> the Jacobians of $h$ and $g$:</p> \[\Jfc = \Jhc \cdot \Jgc \quad .\] <p>Figure 1 illustrates this for $n=5$, $m=4$ and $p=3$. We will keep using these dimensions in following illustrations.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/chainrule_num.svg-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/chainrule_num.svg-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/chainrule_num.svg-1400.webp"></source> <img src="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/chainrule_num.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Figure 1: Visualization of the multivariate chain rule for $f = h \circ g$. </div> <h3 id="ad-is-matrix-free">AD is matrix-free</h3> <p>We have seen how the chain rule translates the compositional structure of a function into the product structure of its Jacobian. Thanks to the small dimensions $n$, $m$ and $p$, this approach worked well on our toy example in Figure 1. In practice however, there is a problem: <strong>materializing intermediate Jacobian matrices is inefficient and often impossible</strong>, especially with a dense matrix format. Examples of dense matrix formats include NumPy’s <code class="language-plaintext highlighter-rouge">ndarray</code>, PyTorch’s <code class="language-plaintext highlighter-rouge">Tensor</code>, JAX’s <code class="language-plaintext highlighter-rouge">Array</code> and Julia’s <code class="language-plaintext highlighter-rouge">Matrix</code>.</p> <p>As a motivating example, let us take a look at a tiny convolutional layer. We consider a convolutional filter of size $5 \times 5$, a single input channel and a single output channel. An input of size $28 \times 28 \times 1$ results in a $576 \times 784$ Jacobian, the structure of which is shown in Figure 2. All the white coefficients are <strong>structural zeros</strong>.</p> <p>If we materialize the entire Jacobian as a dense matrix:</p> <ul> <li>we waste time computing coefficients which are mostly zero;</li> <li>we waste memory storing those zero coefficients.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/big_conv_jacobian-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/big_conv_jacobian-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/big_conv_jacobian-1400.webp"></source> <img src="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/big_conv_jacobian.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Figure 2: Structure of the Jacobian of a tiny convolutional layer. </div> <p>In modern neural network architectures, which can contain over one trillion parameters, computing intermediate Jacobians is not only inefficient: it exceeds available memory. AD circumvents this limitation using <strong>linear maps</strong>, lazy operators that act exactly like matrices but without materializing them.</p> <p>The differential $Df: \vx \longmapsto Df(\vx)$ is a linear map which provides the best linear approximation of $f$ around a given point $\vx$. We can rephrase the chain rule as a <strong>composition of linear maps</strong> instead of a product of matrices:</p> \[\Dfc = \colorf{\D{(h \circ g)}{\vx}} = \Dhc \circ \Dgc .\] <p>Note that all terms in this formulation of the chain rule are linear maps. A new visualization for our toy example can be found in Figure 3b. Our illustrations distinguish between materialized matrices and linear maps by using solid and dashed lines respectively.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/chainrule_num.svg-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/chainrule_num.svg-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/chainrule_num.svg-1400.webp"></source> <img src="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/chainrule_num.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Figure 3a: Chain rule using materialized Jacobians (solid outline). </div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/matrixfree.svg-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/matrixfree.svg-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/matrixfree.svg-1400.webp"></source> <img src="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/matrixfree.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Figure 3b: Chain rule using matrix-free linear maps (dashed outline). </div> <p><em>We visualize “matrix entries” in linear maps to build intuition. Even though following illustrations will sometimes put numbers onto these “matrix entries”, linear maps are best thought of as black-box functions.</em></p> <h3 id="forward-mode-ad">Forward-mode AD</h3> <p>Now that we have translated the compositional structure of our function $f$ into a compositional structure of linear maps, we can evaluate them by propagating <strong>materialized vectors</strong> through them.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/forward_mode_eval.svg-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/forward_mode_eval.svg-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/forward_mode_eval.svg-1400.webp"></source> <img src="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/forward_mode_eval.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Figure 4: Evaluating linear maps in forward-mode. </div> <p>Figure 4 illustrates the propagation of a vector $\vv_1 \in \sR^n$ from the right-hand side. Since we propagate in the order of the original function evaluation, this is called <strong>forward-mode AD</strong>.</p> <p>In the first step, we evaluate $Dg(\vx)(\vv_1)$. Since this operation by definition corresponds to</p> \[\vvc{2} = \Dgc(\vvc{1}) = \Jgc \cdot \vvc{1} \;\in \sR^p ,\] <p>it is also commonly called a <strong>Jacobian-vector product</strong> (JVP) or <strong>pushforward</strong>. The resulting vector $\vv_2$ is then used to compute the subsequent JVP</p> \[\vvc{3} = \Dhc(\vvc{2}) = \Jhc \cdot \vvc{2} \;\in \sR^m ,\] <p>which in accordance with the chain rule is equivalent to</p> \[\vvc{3} = \Dfc(\vvc{1}) = \Jfc \cdot \vvc{1} ,\] <p>the JVP of our composed function $f$.</p> <p><strong>Note that we did not materialize intermediate Jacobians at any point</strong> – we only propagated vectors through linear maps.</p> <h3 id="reverse-mode-ad">Reverse-mode AD</h3> <p>We can also propagate vectors through our linear maps from the left-hand side, resulting in <strong>reverse-mode AD</strong>, shown in Figure 5. Just like forward-mode, reverse-mode is also matrix-free: <strong>no intermediate Jacobians are materialized at any point</strong>.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/reverse_mode_eval.svg-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/reverse_mode_eval.svg-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/reverse_mode_eval.svg-1400.webp"></source> <img src="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/reverse_mode_eval.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Figure 5: Evaluating linear maps in reverse-mode. </div> <h3 id="from-linear-maps-back-to-jacobians">From linear maps back to Jacobians</h3> <p>The linear map formulation allows us to avoid intermediate Jacobian matrices in long chains of function compositions. But can we use this machinery to materialize the <strong>Jacobian</strong> of the composition $f$ itself?</p> <p>As shown in Figure 6, we can <strong>materialize Jacobians column by column</strong> in forward mode. Evaluating the linear map $Df(\vx)$ on the $i$-th standard basis vector materializes the $i$-th column of the Jacobian $J_f(\vx)$:</p> \[\Dfc(\vbc{i}) = \left( \Jfc \right)_\colorv{i,:}\] <p>Thus, materializing the full $m \times n$ Jacobian requires one JVP with each of the $n$ standard basis vectors of the <strong>input space</strong>.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/forward_mode.svg-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/forward_mode.svg-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/forward_mode.svg-1400.webp"></source> <img src="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/forward_mode.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Figure 6: Forward-mode AD materializes Jacobians column-by-column. </div> <p>As illustated in Figure 7, we can also <strong>materialize Jacobians row by row</strong> in reverse mode. Unlike forward mode in Figure 6, this requires one VJP with each of the $m$ standard basis vectors of the <strong>output space</strong>.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/reverse_mode.svg-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/reverse_mode.svg-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/reverse_mode.svg-1400.webp"></source> <img src="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/reverse_mode.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Figure 7: Reverse-mode AD materializes Jacobians row-by-row. </div> <p>Since neural networks are usually trained using scalar loss functions, reverse-mode AD only requires the evaluation of a single VJP to compute a gradient. This makes it the method of choice for machine learners, who typically refer to reverse-mode AD as <em>backpropagation</em>.</p> <h2 id="sparse-automatic-differentiation">Sparse automatic differentiation</h2> <h3 id="sparse-matrices">Sparse matrices</h3> <p>Sparse matrices are matrices in which most elements are zero. We refer to linear maps as “sparse linear maps” if they materialize to sparse matrices.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/sparse_matrix.svg-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/sparse_matrix.svg-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/sparse_matrix.svg-1400.webp"></source> <img src="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/sparse_matrix.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/sparse_map.svg-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/sparse_map.svg-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/sparse_map.svg-1400.webp"></source> <img src="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/sparse_map.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 8: A sparse Jacobian and its corresponding sparse linear map. </div> <p>Whene functions have many inputs and many outputs, a given output does not always depend on every single input. This endows the corresponding Jacobian with a sparse structure, where zero coefficients denote an absence of (first-order) dependency. The previous case of a convolutional layer is a simple example. An even simpler example is an activation function applied elementwise, for which the Jacobian is the identity matrix.</p> <h3 id="leveraging-structure">Leveraging structure</h3> <p>Assuming we know the structure of the Jacobian, we can find orthogonal, non-overlapping columns or rows via a method called <strong>matrix coloring</strong> that we will go into more detail on later.</p> <p><strong>The core idea of ASD is that we can materialize multiple orthogonal columns or rows in a single evaluation.</strong> Since linear maps are additive, it always holds that</p> \[\Dfc(\vbc{i}+\ldots+\vbc{j}) = \underbrace{\Dfc(\vbc{i})}_{\left( \Jfc \right)_\colorv{i,:}} + \ldots + \underbrace{\Dfc(\vbc{j})}_{\left( \Jfc \right)_\colorv{j,:}} .\] <p>The right hand side summands each correspond to a column of the Jacobian. If the columns are <strong>orthogonal</strong> and their <strong>structure is known</strong>, the sum can be decomposed into its summands, materializing multiple columns in a single JVP.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/sparse_ad.svg-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/sparse_ad.svg-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/sparse_ad.svg-1400.webp"></source> <img src="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/sparse_ad.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Figure 9: Materializing multiple orthogonal columns of a Jacobian in forward-mode. </div> <p>This specific example using JVPs corresponds to sparse forward-mode AD and is visualized in Figure 9, where all orthogonal columns have been colored in matching hues. By computing a single JVP with the vector $\mathbf{e}_1 + \mathbf{e}_2 + \mathbf{e}_5$, we materialize the sum of the first, second and fifth column of our Jacobian.</p> <p>Since we can assume we know the structure of the Jacobian, we can assign the values in the resulting vector to the correct Jacobian entries. The full forward-mode ASD materialization of our toy Jacobian is shown in Figure X.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/sparse_ad_forward_full.svg-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/sparse_ad_forward_full.svg-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/sparse_ad_forward_full.svg-1400.webp"></source> <img src="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/sparse_ad_forward_full.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/sparse_ad_forward_decompression.svg-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/sparse_ad_forward_decompression.svg-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/sparse_ad_forward_decompression.svg-1400.webp"></source> <img src="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/sparse_ad_forward_decompression.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure X: Materializing a Jacobian with forward-mode ASD: (a) compressed evaluation of orthogonal columns (b) decompression to Jacobian matrix </div> <p>The same idea can also be applied to reverse mode AD, as shown in Figure Y. Instead of finding orthogonal column, we need to find orthogonal rows. We can then materialize multiple rows in a single VJP.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/sparse_ad_reverse_full.svg-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/sparse_ad_reverse_full.svg-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/sparse_ad_reverse_full.svg-1400.webp"></source> <img src="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/sparse_ad_reverse_full.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/sparse_ad_reverse_decompression.svg-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/sparse_ad_reverse_decompression.svg-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/sparse_ad_reverse_decompression.svg-1400.webp"></source> <img src="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/sparse_ad_reverse_decompression.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure Y: Materializing a Jacobian with reverse-mode ASD: (a) compressed evaluation of orthogonal rows (b) decompression to Jacobian matrix </div> <h3 id="sparsity-pattern-detection-and-coloring">Sparsity pattern detection and coloring</h3> <p>Unfortunately, our initial assumption had a major flaw: Since AD only gives us a composition of linear maps and linear maps are black-box functions, the structure of the Jacobian is completely unknown.</p> <p><strong>We can’t tell which rows and columns are orthogonal without first materializing a Jacobian matrix.</strong> But if we fully materialize a Jacobian via traditional AD, ASD isn’t needed.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/sparsity_pattern.svg-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/sparsity_pattern.svg-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/sparsity_pattern.svg-1400.webp"></source> <img src="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/sparsity_pattern.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/coloring.svg-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/coloring.svg-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/coloring.svg-1400.webp"></source> <img src="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/coloring.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 10: The two elementary steps in ASD: (a) sparsity pattern detection, (b) coloring of the sparsity pattern. </div> <p>The solution to this problem is shown in Figure 10: in order to find orthogonal columns (or rows), we don’t need to materialize the full Jacobian. Instead, it is enough to materialize a binary sparsity pattern of the Jacobian. This pattern contains enough information to color it.</p> <p>Performance is key: For one-off computations, these two steps need to be faster than the computation of columns or rows they allow us to skip. Otherwise, we didn’t gain any performance. As we will see in later benchmarks, this level of performance can be achieved. Additionally, if we need to compute Jacobians multiple times and are able to reuse the sparsity pattern, the cost of sparsity pattern detection and coloring can be amortized over time.</p> <h2 id="pattern-detection">Pattern detection</h2> <p>Sparsity pattern detection can be thought of as a binary version of AD. Mirroring the diversity of existing approaches to AD, there are also many possible approaches to sparsity pattern detection, each with their own advantages and tradeoffs.</p> <p>The method we will present here corresponds to a binary forward-mode AD system in which performance is gained by compressing matrix rows. <em>TODO: Alternatives include Bayesian probing, …</em> </p> <h3 id="compressing-jacobians">Compressing Jacobians</h3> <p>Our goal with sparsity pattern detection is to quickly materialize the binary pattern of the Jacobian. One way to achieve better performance than traditional AD is to compress of rows of matrices to index sets. The $i$-th row of the Jacobian corresponds to</p> \[\big( \Jf \big)_{i,:} = \left[\dfdx{i}{j}\right]_{1 \le j \le n} = \begin{bmatrix} \dfdx{i}{1} &amp; \ldots &amp; \dfdx{i}{n} \end{bmatrix} .\] <p>This can naively be represented in a computer program by computing and storing using the corresponding $n$ first-order partial derivatives. However, since we are only interested in the binary pattern</p> \[\left[\dfdx{i}{j} \neq 0\right]_{1 \le j \le n} ,\] <p>we can instead represent the sparsity pattern of the $i$-th column of a Jacobian by the corresponding <strong>index set of non-zero values</strong></p> \[\left\{j \;\Bigg|\; \dfdx{i}{j} \neq 0\right\} .\] <p>These equivalent sparsity pattern representations are illustrated in Figure 11.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/sparsity_pattern_representations.svg-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/sparsity_pattern_representations.svg-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/sparsity_pattern_representations.svg-1400.webp"></source> <img src="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/sparsity_pattern_representations.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Figure 11: Equivalent sparsity pattern representations: (a) uncompressed matrix, (b) binary pattern, (c) index set (compressed along rows). </div> <p>(Since the method we are about to show is essentially a binary forward-mode AD system, we compress along rows.)</p> <h3 id="propagating-index-sets">Propagating index sets</h3> <p>Figure 12 shows the traditional forward-AD pass we want to avoid: propagating a full identity matrix through a linear map would materialize the Jacobian of $f$, but also all intermediate linear maps. As previously discussed, this is not a viable option due to its inefficiency and high memory requirements.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/forward_mode_naive.svg-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/forward_mode_naive.svg-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/forward_mode_naive.svg-1400.webp"></source> <img src="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/forward_mode_naive.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Figure 12: Materializing a Jacobian forward-mode. Due to high memory requirements for intermediate Jacobians, this approach is inefficient or impossible. </div> <p>Instead, we <em>seed</em> an input vector with index sets corresponding to the compressed identity matrix. An alternative view on this vector is that it corresponds to the index set representation of the Jacobian of the input, since $\frac{\partial x_i}{\partial x_j} \neq 0$ only holds for $i=j$.</p> <p>Our goal is to propagate this index set such that we get an output vector of index sets that corresponds to the Jacobian sparsity pattern. This idea is visualized in Figure 13.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/forward_mode_sparse.svg-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/forward_mode_sparse.svg-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/forward_mode_sparse.svg-1400.webp"></source> <img src="/iclr2025-sparse-autodiff/assets/img/2025-04-28-sparse-autodiff/forward_mode_sparse.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Figure 13: Propagating an index set through a linear map to obtain a sparsity pattern. </div> <h3 id="alternative-evaluation">Alternative evaluation</h3> <p>Instead of going into implementation details, we want to provide some intuition on the second key ingredient of our forward-mode sparsity detection: <strong>alternative function evaluation</strong>.</p> <p>We will demonstrate this on a second toy example, the function</p> \[f(\vx) = x_1 + x_2x_3 + \text{sgn}(x_4) .\] <p>The corresponding computational graph is shown in Figure 14, where circular nodes correspond to elementary operators, in this case addition, multiplication and the sign function.</p> <div class="caption"> Figure 14: Computational graph of the function $ f(\vx) = x_1 + x_2x_3 + \text{sgn}(x_4) $, annotated with corresponding index sets. </div> <p>As discussed in the previous section, all inputs are seeded with their respective input index sets. Figure 14 annotates these index sets on the edges of the computational graph. Our system for sparsity detection must now perform an <strong>alternative evaluation of our computational graph</strong>. Instead of computing the original function, each operator must correctly propagate and accumulate the index sets of its inputs, depending on whether an operator has a non-zero derivative or not.</p> <p>Since addition and multiplication globally have non-zero derivatives with respect to both of their inputs, the index sets of their inputs are accumulated and propagated. The sign function has a zero-valued derivatives for any input value. It therefore doesn’t propagate the index set of its input. Instead, it returns an empty set.</p> <p><em>TODO: switch to multivariate function, quickly discuss resulting Jacobian.</em> </p> <h3 id="matrix-coloring">Matrix coloring</h3> <h2 id="second-order-sparse-differentiation">Second-order sparse differentiation</h2> <h2 id="demonstration">Demonstration</h2> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/iclr2025-sparse-autodiff/assets/bibliography/2025-04-28-sparse-autodiff.bib"></d-bibliography> <d-article id="bibtex-container" class="related highlight"> For attribution in academic contexts, please cite this work as <pre id="bibtex-academic-attribution">
        PLACEHOLDER FOR ACADEMIC ATTRIBUTION
  </pre> BibTeX citation <pre id="bibtex-box">
        PLACEHOLDER FOR BIBTEX
  </pre> </d-article> <script src="https://utteranc.es/client.js" repo="iclr-blogposts/2025" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>